# TEAM-5 Speech Analysis Pipeline

An advanced AI-powered speech analysis system that records audio, transcribes speech, analyzes communication patterns, and generates personalized feedback reports using local LLMs and RAG (Retrieval Augmented Generation).

## Features

- üéôÔ∏è **Audio Recording**: Record audio directly from your microphone
- üîß **Audio Preprocessing**: Automatic noise reduction and normalization
- üìù **Speech-to-Text**: Accurate transcription using Faster-Whisper
- üìä **Speech Analysis**: Comprehensive acoustic feature extraction (pitch, energy, pauses, etc.)
- üß† **AI Agents**: Specialized agents for communication, confidence, and personality analysis
- üîç **RAG System**: Knowledge-enhanced reports using vector database retrieval
- üõ°Ô∏è **Guardrails**: Input/output validation and safety checks
- ‚ú® **Final Report**: AI-generated personalized feedback and recommendations

## System Requirements

- **Python**: 3.8 or higher
- **Operating System**: Linux, macOS, or Windows
- **RAM**: Minimum 8GB recommended
- **Storage**: ~5GB for models and dependencies
- **Microphone**: Required for audio recording

## Installation

### 1. Clone the Repository

```bash
git clone https://github.com/GSMPRANEETH/TEAM-5.git
cd TEAM-5
```

### 2. Create a Virtual Environment (Recommended)

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Install Ollama (Required for LLM)

Ollama is used for local LLM inference. Install it from [ollama.ai](https://ollama.ai/):

**Linux/macOS:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
1. Download and install from [ollama.ai/download](https://ollama.ai/download)
2. **Important**: Add Ollama to your system PATH environment variable:
   - The installer typically installs Ollama to `C:\Users\%USERNAME%\AppData\Local\Programs\Ollama`
   - **Note**: Use `%USERNAME%` exactly as shown - Windows will automatically replace it with your actual username
   - **Option 1 - Add to User Environment Variables:**
     1. Open "Edit environment variables for your account" from Start menu
     2. Under "User variables", select "Path" and click "Edit"
     3. Click "New" and add the path exactly: `C:\Users\%USERNAME%\AppData\Local\Programs\Ollama`
     4. Click "OK" on all dialogs
   - **Option 2 - Add to System Environment Variables (requires admin):**
     1. Open "Edit the system environment variables" from Start menu
     2. Click "Environment Variables"
     3. Under "System variables", select "Path" and click "Edit"
     4. Click "New" and add the path exactly: `C:\Users\%USERNAME%\AppData\Local\Programs\Ollama`
     5. Click "OK" on all dialogs
3. **Restart your terminal/command prompt** after adding to PATH
4. Verify installation: Open a new terminal and run `ollama --version`

### 5. Pull the LLM Model

After installing Ollama, pull the required model:

```bash
ollama pull llama3.2:3b
```

The default model is `llama3.2:3b`. You can change this in `llm1/llm_config.py`.

### 6. (Optional) Install GuardrailsAI for Enhanced Safety

GuardrailsAI provides additional input/output validation:

```bash
pip install guardrails-ai
guardrails configure
```

Install Hub validators (optional but recommended):

```bash
guardrails hub install hub://guardrails/toxic_language
guardrails hub install hub://guardrails/profanity_free
guardrails hub install hub://guardrails/detect_pii
```

## Configuration

### LLM Configuration

Edit `llm1/llm_config.py` to customize LLM settings:

```python
LLM_MODEL_NAME = "llama3.2:3b"  # Change to your preferred model
TEMPERATURE = 0.3                # Creativity level (0.0-1.0)
MAX_TOKENS = 2048               # Maximum response length
```

### RAG Configuration

Edit `rag/config.py` to customize retrieval settings:

```python
CHROMA_PERSIST_DIR = "./chroma_db"  # Vector database storage
TOP_K_RESULTS = 3                    # Number of documents to retrieve
```

### Recording Settings

Edit the configuration in `main.py`:

```python
DURATION = 45        # Recording duration in seconds
SAMPLE_RATE = 16000  # Audio sample rate (required for Whisper)
CHANNELS = 1         # Mono audio
```

## Usage

### Basic Usage - Full Pipeline

Run the complete pipeline (record, transcribe, analyze, and report):

```bash
python main.py
```

This will:
1. Record audio for 45 seconds (speak into your microphone)
2. Preprocess and clean the audio
3. Transcribe speech to text
4. Analyze speech features
5. Run AI agents for analysis
6. Generate a personalized AI report

### Testing Individual Components

#### Test Speech-to-Text Only

```bash
# Requires clean_audio.wav file
python speech_to_text.py
```

#### Test Speech Analysis Only

```bash
# Requires clean_audio.wav file
python -c "from pipeline import get_pipeline_output; print(get_pipeline_output('clean_audio.wav'))"
```

#### Test Agent System

```bash
python agent.py
```

#### Test RAG System

```bash
python test_rag.py
```

#### Test LLM Connection

```bash
python test_llm_step5.py
```

### Processing Existing Audio

To analyze an existing audio file instead of recording:

1. Place your audio file as `raw_audio.wav` in the project directory
2. Comment out the `record_audio()` call in `main.py`
3. Run `python main.py`

Or use the pipeline module directly:

```python
from pipeline import get_pipeline_output

# Analyze your audio file
result = get_pipeline_output("your_audio.wav")
print(result)
```

## Project Structure

```
TEAM-5/
‚îú‚îÄ‚îÄ main.py                 # Main entry point - full pipeline
‚îú‚îÄ‚îÄ agent.py                # Agent orchestrator
‚îú‚îÄ‚îÄ speech_to_text.py       # Whisper transcription
‚îú‚îÄ‚îÄ speech_features.py      # Acoustic feature extraction
‚îú‚îÄ‚îÄ pipeline.py             # Pipeline utilities
‚îú‚îÄ‚îÄ llm_helper.py           # LLM loader helper
‚îú‚îÄ‚îÄ guardrails_config.py    # Input/output validation
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ README.md              # This file
‚îÇ
‚îú‚îÄ‚îÄ agents/                # Specialized AI agents
‚îÇ   ‚îú‚îÄ‚îÄ communication_agent.py
‚îÇ   ‚îú‚îÄ‚îÄ confidence_agent.py
‚îÇ   ‚îî‚îÄ‚îÄ personality_agent.py
‚îÇ
‚îú‚îÄ‚îÄ llm/                   # LLM wrapper (for agents)
‚îÇ   ‚îî‚îÄ‚îÄ local_llm.py
‚îÇ
‚îú‚îÄ‚îÄ llm1/                  # LLM configuration & report generation
‚îÇ   ‚îú‚îÄ‚îÄ llm_config.py
‚îÇ   ‚îú‚îÄ‚îÄ local_llm.py
‚îÇ   ‚îú‚îÄ‚îÄ prompt_templates.py
‚îÇ   ‚îî‚îÄ‚îÄ report_generator.py
‚îÇ
‚îú‚îÄ‚îÄ rag/                   # RAG system
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py
‚îÇ   ‚îú‚îÄ‚îÄ knowledge_base.py
‚îÇ   ‚îú‚îÄ‚îÄ rag_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ documents/         # Knowledge base documents
‚îÇ       ‚îú‚îÄ‚îÄ communication_rules.md
‚îÇ       ‚îú‚îÄ‚îÄ confidence_psychology.md
‚îÇ       ‚îî‚îÄ‚îÄ personality_traits.md
‚îÇ
‚îî‚îÄ‚îÄ utils/                 # Utilities
    ‚îî‚îÄ‚îÄ parser.py          # JSON parsing utilities
```

## Dependencies

### Core Dependencies

- **torch** & **torchaudio**: Deep learning framework for audio processing
- **faster-whisper**: Efficient speech-to-text transcription
- **librosa**: Audio analysis and processing
- **sounddevice** & **soundfile**: Audio recording and I/O
- **opensmile**: Acoustic feature extraction
- **pyannote.audio**: Speaker diarization and audio analysis

### LLM & Agent Framework

- **langchain**: LLM orchestration framework
- **langchain-ollama**: Ollama integration for LangChain
- **ollama**: Local LLM runtime

### RAG System

- **chromadb**: Vector database for semantic search
- **transformers**: HuggingFace models for embeddings
- **sentence-transformers**: Efficient sentence embeddings

### Validation & Safety

- **guardrails-ai**: Input/output validation framework
- **pydantic**: Data validation

See `requirements.txt` for complete dependency list with versions.

## Troubleshooting

### Ollama Connection Issues

**Error**: `Ollama not available` or `ollama: command not found`

**Solution**:
1. Ensure Ollama is installed and running: `ollama serve`
2. Check if the model is pulled: `ollama list`
3. Pull the model if missing: `ollama pull llama3.2:3b`
4. **Windows users**: If you get `command not found`, verify Ollama is in your PATH:
   - Open a new terminal and run `ollama --version`
   - If it fails, add Ollama to PATH (see installation step 4 above)
   - Use the path with %USERNAME% variable: `C:\Users\%USERNAME%\AppData\Local\Programs\Ollama`
   - Windows will automatically expand %USERNAME% to your actual username
   - Restart your terminal after adding to PATH
5. **Linux/macOS users**: If Ollama isn't in PATH, it may be installed in a custom location:
   - Try: `which ollama` to find the installation path
   - Add it to PATH: `export PATH=$PATH:/path/to/ollama`
   - Add to `~/.bashrc` or `~/.zshrc` to make permanent

The system will fall back to a stub LLM if Ollama is unavailable, which returns mock data for testing.

### Microphone Issues

**Error**: Audio recording fails

**Solution**:
1. Check microphone permissions for your terminal/Python
2. List available audio devices: `python -c "import sounddevice; print(sounddevice.query_devices())"`
3. Update device ID in recording code if needed

### ChromaDB Issues

**Error**: ChromaDB initialization fails

**Solution**:
1. Ensure ChromaDB is installed: `pip install chromadb`
2. Clear the database: `rm -rf chroma_db/`
3. The system will fall back to keyword-based retrieval if ChromaDB is unavailable

### Import Errors

**Error**: Module not found

**Solution**:
1. Ensure virtual environment is activated
2. Reinstall dependencies: `pip install -r requirements.txt`
3. Check Python version: `python --version` (should be 3.8+)

### Memory Issues

**Error**: Out of memory during processing

**Solution**:
1. Use a smaller LLM model in `llm1/llm_config.py`
2. Reduce `MAX_TOKENS` in the configuration
3. Close other memory-intensive applications

## Advanced Configuration

### Using a Different LLM Model

Edit `llm1/llm_config.py`:

```python
# Available models (example):
LLM_MODEL_NAME = "llama3.2:3b"     # 3B parameters (fastest)
# LLM_MODEL_NAME = "llama3.2:7b"   # 7B parameters (balanced)
# LLM_MODEL_NAME = "llama3.2:13b"  # 13B parameters (most accurate)
```

Pull the new model:
```bash
ollama pull llama3.2:7b
```

### Customizing Knowledge Base

Add your own documents to the RAG system:

1. Create markdown files in `rag/documents/`
2. Restart the system to re-index

### Adjusting Recording Duration

Edit `main.py`:

```python
DURATION = 60  # Record for 60 seconds instead of 45
```

## Performance Tips

1. **Use GPU acceleration**: If available, install PyTorch with CUDA support
2. **Optimize Whisper**: Use smaller model sizes for faster transcription
3. **Reduce context length**: Decrease `TOP_K_RESULTS` in RAG config for faster retrieval
4. **Pre-download models**: Download all models before first run to avoid delays

## Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## License

[Add license information here]

## Acknowledgments

- **Faster-Whisper**: OpenAI Whisper implementation by Guillaume Klein
- **Ollama**: Local LLM runtime by Ollama Team
- **LangChain**: LLM application framework
- **ChromaDB**: Vector database for AI applications
- **GuardrailsAI**: LLM output validation framework

## Support

For issues, questions, or contributions:
- Open an issue on GitHub
- Contact the development team

## Changelog

### Current Version
- Full speech analysis pipeline
- RAG-enhanced AI reporting
- Multi-agent analysis system
- GuardrailsAI integration
- Local LLM support via Ollama
